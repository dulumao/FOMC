"""Convert raw news search results into structured macro events."""

from __future__ import annotations

from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import date
import json
import re
from typing import Dict, List, Optional, Sequence

from .config import HIGH_TRUST_DOMAINS, IMPACT_CHANNELS, MACRO_SHOCK_TYPES
from .duckduckgo_client import NewsItem


@dataclass
class MacroEventCandidate:
    date: str  # "YYYY-MM-DD"
    macro_shock_type: str
    impact_channel: List[str]
    countries: List[str]
    summary_zh: str
    source_title: str
    source_url: str
    source_domain: str
    is_primary: bool


def _classify_text(text: str, report_type: str) -> tuple[str, List[str]]:
    text_lower = text.lower()
    if any(keyword in text_lower for keyword in ["tariff", "trade war", "export control", "sanction"]):
        shock = "trade_tariff" if "tariff" in text_lower or "trade war" in text_lower else "sanctions"
    elif any(keyword in text_lower for keyword in ["port", "shipping", "supply chain", "logistics", "shipment", "canal"]):
        shock = "supply_chain"
    elif any(keyword in text_lower for keyword in ["strike", "walkout", "labor dispute", "uaw", "union"]):
        shock = "labor_dispute"
    elif any(keyword in text_lower for keyword in ["bank failure", "bank collapse", "liquidity", "bank run", "credit crunch"]):
        shock = "financial_stability"
    else:
        shock = "other"

    impact: List[str] = []
    if "inflation" in text_lower or "price" in text_lower or report_type == "cpi":
        impact.append("inflation")
    if "job" in text_lower or "employment" in text_lower or report_type == "nfp":
        impact.append("employment")
    if any(keyword in text_lower for keyword in ["gdp", "growth", "output", "factory"]):
        impact.append("growth")
    if any(keyword in text_lower for keyword in ["credit", "liquidity", "bank", "yields", "bonds"]):
        impact.append("financial_conditions")
    if not impact:
        impact.append("growth")
    # keep ordering and deduplicate
    seen = set()
    impact_dedup: List[str] = []
    for channel in impact:
        if channel in seen:
            continue
        if channel in IMPACT_CHANNELS:
            impact_dedup.append(channel)
            seen.add(channel)
    return shock, impact_dedup


def _infer_countries(text: str) -> List[str]:
    text_lower = text.lower()
    countries: List[str] = []
    if any(keyword in text_lower for keyword in ["united states", "u.s.", "usa", "us "]):
        countries.append("US")
    if "china" in text_lower or "chinese" in text_lower or "prc" in text_lower:
        countries.append("CN")
    if "europe" in text_lower or "euro" in text_lower:
        countries.append("EU")
    if not countries:
        countries.append("US")
    return countries


def filter_and_classify_news(
    news_list: List[NewsItem],
    report_type: str,
    start_date: date | None = None,
    end_date: date | None = None,
) -> List[MacroEventCandidate]:
    """
    Filter NewsItem list and attach coarse labels for later clustering.
    """
    seen_urls = set()
    candidates: List[MacroEventCandidate] = []
    for item in news_list:
        if item.url in seen_urls:
            continue
        seen_urls.add(item.url)

        if start_date and end_date and item.published_at:
            if not (start_date <= item.published_at <= end_date):
                continue

        # For important links prefer full_text; otherwise use snippet/body.
        snippet_or_body = item.full_text if getattr(item, "is_primary", False) and item.full_text else (item.snippet or "")
        text_blob = " ".join(filter(None, [item.title, snippet_or_body])).strip()
        if not text_blob:
            continue

        macro_shock_type, impact_channel = _classify_text(text_blob, report_type)
        countries = _infer_countries(text_blob)

        # Store raw content for downstream clustering/summarization:
        # primary -> use full_text (truncated), supplementary -> use snippet/body
        content_for_summary = snippet_or_body if snippet_or_body else item.title
        if content_for_summary and len(content_for_summary) > 500:
            content_for_summary = content_for_summary[:500] + "..."
        summary_zh = f"{item.source or '来源未知'}：{content_for_summary}"
        candidates.append(
            MacroEventCandidate(
                date=item.published_at.isoformat() if item.published_at else "",
                macro_shock_type=macro_shock_type,
                impact_channel=impact_channel,
                countries=countries,
                summary_zh=summary_zh,
                source_title=item.title,
                source_url=item.url,
                source_domain=item.source or "",
                is_primary=getattr(item, "is_primary", False),
            )
        )
    return candidates


def _bucket_key(candidate: MacroEventCandidate) -> tuple:
    impact = candidate.impact_channel[0] if candidate.impact_channel else "growth"
    country = candidate.countries[0] if candidate.countries else "US"
    return (candidate.macro_shock_type, impact, country)


def _bucket_candidates(candidates: Sequence[MacroEventCandidate]) -> Dict[tuple, List[MacroEventCandidate]]:
    buckets: Dict[tuple, List[MacroEventCandidate]] = defaultdict(list)
    for cand in candidates:
        buckets[_bucket_key(cand)].append(cand)
    return buckets


def _normalize_title(title: str) -> str:
    if not title:
        return ""
    text = re.sub(r"[^a-z0-9]+", " ", title.lower())
    return re.sub(r"\s+", " ", text).strip()


def _extract_json_list(text: str) -> Optional[List]:
    try:
        data = json.loads(text)
        if isinstance(data, dict) and isinstance(data.get("clusters"), list):
            return data.get("clusters")
        if isinstance(data, list):
            return data
    except Exception:
        pass
    start = text.find("[")
    end = text.rfind("]")
    if start >= 0 and end > start:
        try:
            return json.loads(text[start : end + 1])
        except Exception:
            return None
    return None


def _cluster_items_with_llm(items: List[Dict]) -> Optional[List[List[int]]]:
    try:
        from .llm_client import call_llm
    except Exception:
        return None
    prompt = (
        "你是一名新闻事件聚类器。给定若干条新闻条目，请按“是否描述同一宏观事件”聚类。"
        "必须覆盖全部索引且不重复。输出 JSON 数组，每个元素包含 members 列表。"
        "示例: [{\"members\":[0,2]}, {\"members\":[1]}]。不要解释。\n\n条目：\n"
    )
    body = prompt + json.dumps(items, ensure_ascii=True)
    resp = call_llm(
        [
            {"role": "system", "content": "你只返回 JSON。"},
            {"role": "user", "content": body},
        ],
        max_tokens=400,
    )
    data = _extract_json_list(resp.strip())
    if not data:
        return None
    clusters: List[List[int]] = []
    for entry in data:
        if isinstance(entry, dict):
            members = entry.get("members")
        else:
            members = entry
        if not isinstance(members, list):
            continue
        idxs = [i for i in members if isinstance(i, int)]
        if idxs:
            clusters.append(idxs)
    if not clusters:
        return None
    return clusters


def _merge_cluster(cands: List[MacroEventCandidate]) -> Dict:
    dates = [c.date for c in cands if c.date]
    date_value = min(dates) if dates else ""
    shock_counts = Counter(c.macro_shock_type for c in cands if c.macro_shock_type)
    macro_shock_type = shock_counts.most_common(1)[0][0] if shock_counts else "other"
    impact_channels = sorted({ch for c in cands for ch in c.impact_channel})
    countries = sorted({cty for c in cands for cty in c.countries})
    summaries = [c.summary_zh for c in cands if c.summary_zh]
    source_meta = [
        {
            "title": c.source_title,
            "url": c.source_url,
            "domain": c.source_domain,
            "important": c.is_primary,
        }
        for c in cands
    ]
    domains_all = [s.get("domain") for s in source_meta if s.get("domain")]
    domain_bonus = sum(1 for d in domains_all if d in HIGH_TRUST_DOMAINS)
    importance_score = len(source_meta) + 0.5 * domain_bonus
    primary_meta = [s for s in source_meta if s.get("important")]
    supp_meta = [s for s in source_meta if not s.get("important")]
    source_titles = [s.get("title") for s in source_meta]
    source_urls = [s.get("url") for s in source_meta]
    source_domains = domains_all
    title = ""
    for meta in primary_meta + source_meta:
        if meta.get("title"):
            title = meta["title"]
            break
    return {
        "date": date_value,
        "macro_shock_type": macro_shock_type,
        "impact_channel": impact_channels,
        "countries": countries,
        "importance_score": importance_score,
        "title": title,
        "summary_zh": "；".join(summaries),
        "summary_en": None,
        "source_titles": source_titles,
        "source_urls": source_urls,
        "source_domains": source_domains,
        "source_meta": source_meta,
        "primary_urls": [s.get("url") for s in primary_meta if s.get("url")],
        "supplementary_urls": [s.get("url") for s in supp_meta if s.get("url")],
    }


def _fallback_cluster(candidates: List[MacroEventCandidate]) -> List[Dict]:
    clusters: Dict[str, List[MacroEventCandidate]] = defaultdict(list)
    for cand in candidates:
        key = _normalize_title(cand.source_title) or cand.source_url
        clusters[key].append(cand)
    return [_merge_cluster(cands) for cands in clusters.values()]


def _cluster_bucket(candidates: List[MacroEventCandidate], max_bucket: int = 24) -> List[Dict]:
    if not candidates:
        return []
    items = [
        {
            "idx": idx,
            "title": cand.source_title,
            "summary": cand.summary_zh,
            "source": cand.source_domain,
        }
        for idx, cand in enumerate(candidates)
    ]
    if len(items) <= max_bucket:
        clusters = _cluster_items_with_llm(items)
        if clusters:
            return [_merge_cluster([candidates[i] for i in cluster if 0 <= i < len(candidates)]) for cluster in clusters]
        return _fallback_cluster(candidates)

    chunked_clusters: List[List[MacroEventCandidate]] = []
    for start in range(0, len(items), max_bucket):
        chunk_items = items[start : start + max_bucket]
        chunk_candidates = candidates[start : start + max_bucket]
        clusters = _cluster_items_with_llm(chunk_items)
        if not clusters:
            chunked_clusters.extend([[c] for c in chunk_candidates])
            continue
        for cluster in clusters:
            chunked_clusters.append([chunk_candidates[i] for i in cluster if 0 <= i < len(chunk_candidates)])

    rep_items: List[Dict] = []
    for idx, cluster in enumerate(chunked_clusters):
        rep_title = cluster[0].source_title if cluster else ""
        rep_summary = "；".join(c.summary_zh for c in cluster if c.summary_zh)[:240]
        rep_items.append({"idx": idx, "title": rep_title, "summary": rep_summary, "source": cluster[0].source_domain if cluster else ""})

    rep_clusters = _cluster_items_with_llm(rep_items)
    if not rep_clusters:
        return [_merge_cluster(cluster) for cluster in chunked_clusters]

    merged: List[Dict] = []
    for rep_cluster in rep_clusters:
        merged_candidates: List[MacroEventCandidate] = []
        for rep_idx in rep_cluster:
            if 0 <= rep_idx < len(chunked_clusters):
                merged_candidates.extend(chunked_clusters[rep_idx])
        if merged_candidates:
            merged.append(_merge_cluster(merged_candidates))
    return merged


def cluster_candidates(candidates: List[MacroEventCandidate], use_llm: bool = True) -> List[Dict]:
    """
    Merge similar candidates into macro events using LLM clustering with bucketing.
    """
    if not candidates:
        return []
    if not use_llm:
        return _fallback_cluster(candidates)
    buckets = _bucket_candidates(candidates)
    merged: List[Dict] = []
    for bucket_candidates in buckets.values():
        merged.extend(_cluster_bucket(bucket_candidates))
    return merged


def select_top_events(clustered_events: List[Dict], max_events: int = 10) -> List[Dict]:
    """
    Sort events by importance and return the top N (adaptive: score threshold + cap).
    """
    sorted_events = sorted(
        clustered_events,
        key=lambda e: (-e.get("importance_score", 0), e.get("date") or ""),
    )
    if not sorted_events:
        return []
    scores = [e.get("importance_score", 0) for e in sorted_events]
    # Simple adaptive rule: keep those above max(1.0, 75th percentile) but at least 5, at most max_events.
    scores_sorted = sorted(scores)
    pct75 = scores_sorted[int(0.75 * (len(scores_sorted) - 1))]
    threshold = max(1.0, pct75)
    kept = [e for e in sorted_events if e.get("importance_score", 0) >= threshold]
    if len(kept) < 5:
        kept = sorted_events[:5]
    return kept[:max_events]


def enrich_events_with_llm(events: List[Dict], report_type: str, use_llm: bool = True, model: Optional[str] = None) -> List[Dict]:
    """
    Call LLM to improve summary_zh and reorder by importance.
    """
    if not events:
        return events
    if not use_llm:
        return events
    try:
        from .llm_client import summarize_events_with_llm, llm_rank_and_filter
    except Exception:
        return events
    try:
        enriched = summarize_events_with_llm(events, report_type=report_type, model=model)
        ranked = llm_rank_and_filter(enriched, model=model)
        return ranked
    except Exception:
        return events
